{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd88feb-ec7b-46f4-9d97-595144644360",
   "metadata": {
    "id": "7bd88feb-ec7b-46f4-9d97-595144644360",
    "tags": []
   },
   "source": [
    "# Lecture 4 - Demo Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecb068-bbe6-4e8f-8440-2c7da2001e44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prelimilaries: Imports and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea0334-8272-4fef-8551-9ac42697a4d2",
   "metadata": {
    "id": "aaea0334-8272-4fef-8551-9ac42697a4d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = \"./../../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2601419-738e-4cb8-8583-25efedb7afe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46ccf0-940e-4091-ad03-7bd68c36d7ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820dc934-c038-4045-866b-41f4660e84fc",
   "metadata": {
    "id": "820dc934-c038-4045-866b-41f4660e84fc",
    "outputId": "0fa62836-dbbc-4d1e-d0d5-d43a60e60cd5"
   },
   "outputs": [],
   "source": [
    "# Parse the aggregated data frame\n",
    "df_lq = pd.read_csv('{}/aggregated_extended_fc.csv'.format(DATA_DIR))\n",
    "ts = pd.read_csv('{}/time_series_extended_fc.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b81c8-69c2-4df6-b798-148feebba5e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5352d09-8132-429a-b516-2e673b15c086",
   "metadata": {
    "tags": []
   },
   "source": [
    "### We remove inactive students that did not click during weekdays and weekend for the first 5 weeks of the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactive_students(df, ts):\n",
    "    df = df.fillna('NaN')\n",
    "    \n",
    "    #find all users weeks with 0 clicks on weekends and 0 clicks on weekdays during the first weeks of the semester\n",
    "    df_first = ts[ts.week < 5]\n",
    "    rows = np.where(np.logical_and(df_first.ch_total_clicks_weekend==0, df_first.ch_total_clicks_weekday == 0).to_numpy())[0]\n",
    "    df_zero = df_first.iloc[rows,:]\n",
    "    dropusers = np.unique(df_zero.user)\n",
    "\n",
    "    ts = ts[ts.user.isin(dropusers)==False]\n",
    "    df = df[df.user.isin(dropusers)==False]\n",
    "    return df, ts\n",
    "\n",
    "df_lq, ts = remove_inactive_students(df_lq, ts)\n",
    "# print(df_lq.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfbfe7-df4a-4f2b-a3fb-90193c962105",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_lq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2908b-b4a0-4c94-906b-79eff4767bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064a4fa-f32c-4734-89e3-375b6f4cfc19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare data for classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51e8f1-0122-4cb4-93a9-f8fe343401fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add a pass/fail label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c8ff0-317f-4380-9f35-e42f361f4b77",
   "metadata": {
    "id": "b64efa4d-9168-4a0b-85ee-22fd113c8b9d"
   },
   "outputs": [],
   "source": [
    "# We first add a column to the dataframe containing the outcome variable\n",
    "# compute pass/fail label\n",
    "df_lq['passed'] = df_lq.grade >= 4\n",
    "df_lq['passed'] = df_lq['passed'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b23c3-7225-49bc-9387-3dc62d13faa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove \"bad\" features and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684887bc-4bd2-4f8a-959d-4499d586698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then split the data in a train-test split (stratified by the outcome variable)\n",
    "X = df_lq.drop(['user','grade', 'gender', 'category', 'year', 'passed'], axis=1)\n",
    "y = df_lq['passed']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y) # split train and validation data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a67d0b-7911-4e0a-b29f-f45fcc5f8814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print pass/fail proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac037d43-434d-461d-bc2e-c4a7d74213b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class proportions in train and validation sets are the same, thanks to the stratification on y\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7900ad-42db-440e-83e7-43e385c4924e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Evaluation Metrics (will see later in the slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(clf, X_train, y_train, X_test, y_test, roundnum = 3):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy =  balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    return round(accuracy,roundnum), round(auc,roundnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d1de7-823c-4f22-b989-379aa447d38a",
   "metadata": {
    "id": "cd9d1de7-823c-4f22-b989-379aa447d38a",
    "tags": []
   },
   "source": [
    "# Section 2: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5f0d7-f032-4b51-b640-79127de96bd9",
   "metadata": {
    "id": "54a5f0d7-f032-4b51-b640-79127de96bd9",
    "tags": []
   },
   "source": [
    "### Compute a decision tree of max depth 2  over all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afd964-6041-4e43-a146-4a404f6f38cf",
   "metadata": {
    "id": "b0afd964-6041-4e43-a146-4a404f6f38cf",
    "outputId": "a884b797-9903-4529-bda6-c19141e9eac7"
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=2, random_state=0, criterion='entropy')\n",
    "accuracy, auc = compute_scores(clf, X_train, y_train, X_val, y_val)\n",
    "print(\"Decision tree. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3ba35-b635-45b1-920e-6e597301652a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c10e2-f8e9-4ec9-8923-67f023f47e8c",
   "metadata": {
    "id": "b30c10e2-f8e9-4ec9-8923-67f023f47e8c",
    "outputId": "a0ecb3bd-b465-4ffc-c7d9-c3aa584171fc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(clf, feature_names=X_train.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8177e5e-5be7-4bb5-a091-66fe5ab51b72",
   "metadata": {
    "id": "b8177e5e-5be7-4bb5-a091-66fe5ab51b72",
    "tags": []
   },
   "source": [
    "### Does depth improves perfromance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce41360-e3a9-4f60-b56d-e79e47d330be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the max depth\n",
    "accuracy_list = []\n",
    "auc_list = []\n",
    "for depth in range(1,len(X_train.columns)):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth, random_state=0, criterion='entropy')\n",
    "    accuracy, auc = compute_scores(clf, X_train, y_train, X_val, y_val)\n",
    "    accuracy_list.append(accuracy)\n",
    "    auc_list.append(auc)\n",
    "    # print(\"Decision tree. Depth = {}, Balanced Accuracy = {}, AUC = {}\".format(depth, accuracy, auc))\n",
    "x = list(range(1,len(X_train.columns)))\n",
    "plt.plot(x, accuracy_list, 'r', label = 'accuraÿ≤y')\n",
    "plt.plot(x, auc_list, 'b', label = 'auc')\n",
    "plt.xlabel(\"Decision tree Depth\")\n",
    "plt.ylabel(\"EvaluationMetrics\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473100f-6ae4-40d4-9849-9614c94c252e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 3: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75cf7a0-0401-49c4-a9f3-341122fd35c8",
   "metadata": {
    "id": "b75cf7a0-0401-49c4-a9f3-341122fd35c8"
   },
   "source": [
    "Next, we will use a random forest classifier instead of a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d11e43-26cd-45e7-bab3-9a297328ca6e",
   "metadata": {
    "id": "79d11e43-26cd-45e7-bab3-9a297328ca6e",
    "outputId": "144cea19-fe5f-4931-8614-ad23b0cdc956"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, criterion='entropy') # create a Random Forest\n",
    "accuracy, auc = compute_scores(rf, X_train, y_train, X_val, y_val)\n",
    "print(\"Random Forest. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a82e9c-1c1c-4803-84b4-f15a2b159d6e",
   "metadata": {
    "id": "69a82e9c-1c1c-4803-84b4-f15a2b159d6e"
   },
   "source": [
    "For a single tree, in fact, keeping a low depth is necessary to avoid overfitting and to reduce the variance. Random forests, instead, can have a higher depth, and consequently a lower bias, since the variance is reduced in the aggregation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b573c2-5039-46fe-a23c-11630c2dcba7",
   "metadata": {},
   "source": [
    "In this case, decision trees seem to perform better than random forests. A reason for this behavior could be that the single tree is already very \"stable\", i.e. it will change a little in response to little changes in the data. If this was the case, the submodels in the ensemble forest would be all very similar to the single tree, if they were allowed to choose among all the features at every split. Since, though, only a random subset of features is considered at each split, some subtrees would choose bad splits and have overall bad performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55d1b1-e75d-4299-801f-9c59c60c9ea2",
   "metadata": {
    "id": "742c3c3f-4c79-412a-8db4-e4e56134ff20",
    "tags": []
   },
   "source": [
    "# Section 4: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b4c9e-be3a-4d40-bc69-3f927298dc1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "We only use the euclidean distance since all our features are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc64a8-c652-4732-af20-9d8123497106",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'ch_time_in_prob_sum'\n",
    "\n",
    "# Compute the pairwise distance matrix for all the elements of the training set\n",
    "X_train_dist = squareform(pdist(X_train[feature].to_numpy().reshape(-1,1), metric='euclidean'))\n",
    "\n",
    "# Compute the distance between all elements of the training set and of the validation set\n",
    "X_val_dist = cdist(X_val[feature].to_numpy().reshape(-1,1), X_train[feature].to_numpy().reshape(-1,1), metric='euclidean')\n",
    "\n",
    "X_train_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbc4a0-5bf6-4306-9c05-59f85fd5aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set size:', X_train.shape)\n",
    "print('Validation set size:', X_val.shape)\n",
    "print('Training pairwise distances size:', X_train_dist.shape)\n",
    "print('Validation distances size:', X_val_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00c348-18a8-4968-9b35-61e2168c168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric='precomputed')\n",
    "\n",
    "accuracy, auc = compute_scores(knn, X_train_dist, y_train, X_val_dist, y_val)\n",
    "print(\"k-nearest neighbors. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c8076-f4d2-4b39-bd4c-6146a05387f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 5: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a754c-dd4a-40a8-a57b-2871de1885b3",
   "metadata": {
    "id": "e19a754c-dd4a-40a8-a57b-2871de1885b3",
    "tags": []
   },
   "source": [
    "We normalize the data data using the MinMaxScaler such that all the features are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07ec97-4863-4719-afec-bb70752e81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled  = scaler.transform(X_val)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "accuracy, auc = compute_scores(clf, X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "print(\"Logistic Regression. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30402283-c3a5-4a5b-9b2a-1ead4d045d5a",
   "metadata": {
    "id": "30402283-c3a5-4a5b-9b2a-1ead4d045d5a",
    "tags": []
   },
   "source": [
    "# Section 6: Time Series - Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e35fc4-20c8-44f6-b64b-5ca9aad8340a",
   "metadata": {
    "id": "c2e35fc4-20c8-44f6-b64b-5ca9aad8340a"
   },
   "source": [
    "Build a classifier that can predict whether students pass the course after half of the course (5 weeks). You will need to use the data frame **ts** for this task. You can use kNN, RF, or decision tree. Train your model on the training data and predict on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def862b2-afb5-42c7-8db5-0fc8d5b1103a",
   "metadata": {
    "id": "def862b2-afb5-42c7-8db5-0fc8d5b1103a"
   },
   "source": [
    "We first drop all the weeks before week 5 and then perform the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edb144-3635-4b2d-ab77-94cdf8b4c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only data up to the 5th week\n",
    "ts = ts[ts.week <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c3c07-c2b5-4b35-b8d9-e44aecb3ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split done on the users, so that all the rows corresponding to one user go into the same set.\n",
    "users = ts.user.unique()\n",
    "y = df_lq.passed\n",
    "users_train, users_val, y_train, y_val = train_test_split(users, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "X_train = ts[ts.user.isin(users_train)]\n",
    "X_val = ts[ts.user.isin(users_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4951177-aa43-4e4b-9b39-b8efa2042c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort indexes to make label arrays consistent with the data\n",
    "y_train = y_train.sort_index()\n",
    "y_val = y_val.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e0147-75fe-45e2-bae6-1c79e85fe0ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision tree/Random forest\n",
    "For decision tree and RF, we need to aggregate the features to be able to feed them directly in the mode. We use the mean as an aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49943b-fb15-4cce-b354-f538808329ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features\n",
    "\n",
    "groups_train = X_train.drop('week', axis = 1).groupby('user', as_index = False)\n",
    "groups_val = X_val.drop('week', axis = 1).groupby('user', as_index = False)\n",
    "\n",
    "standard_train = groups_train.std()\n",
    "averages_train = groups_train.mean()\n",
    "standard_val = groups_val.std()\n",
    "averages_val = groups_val.mean()\n",
    "\n",
    "X_train_aggregate = pd.concat([standard_train, averages_train], axis=1)\n",
    "X_val_aggregate = pd.concat([standard_val, averages_val], axis=1)\n",
    "X_train_aggregate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadafbc-1976-43e6-b0a9-d0126c28e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=2, \n",
    "                                  random_state=0, criterion='entropy')\n",
    "\n",
    "accuracy, auc = compute_scores(clf, X_train_aggregate, y_train, X_val_aggregate, y_val)\n",
    "print(\"Decision tree. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07a283-d2d2-467c-a6f6-6b88f55344e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0, criterion='entropy') # create a Random Forest\n",
    "\n",
    "accuracy, auc = compute_scores(rf,X_train_aggregate, y_train, X_val_aggregate, y_val)\n",
    "print(\"Random Forest. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd6cdc-8bc5-4a55-a0a9-01158cd0b17d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-Nearest Neighbors\n",
    "For kNN, we don't need to aggregate the features. We directly compute the pairwise Euclidean distance between users, separately for each feature. We then normalize the pairwise distance matrices such that we can sum them up and feed them directly into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb7e2f-cf23-4038-a474-38131163584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['ch_time_in_prob_sum', 'ma_competency_anti','bo_delay_lecture']\n",
    "\n",
    "X_train_vectors = X_train.groupby('user')[feature].agg(list)\n",
    "X_val_vectors = X_val.groupby('user')[feature].agg(list)\n",
    "\n",
    "X_train_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ae43d-efab-45a3-9755-a533611eed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(distance_matrix):\n",
    "    range_matrix = np.max(distance_matrix) - np.min(distance_matrix)\n",
    "    normalized = (distance_matrix - np.min(distance_matrix)) / range_matrix\n",
    "    return normalized\n",
    "\n",
    "# Compute the pairwise distance matrix for all the elements of the training set, by computing\n",
    "# the distances between the vectors, for each of the features selected, and summing up\n",
    "X_train_dist = sum(map(lambda x: normalize(squareform(pdist(x[1].tolist(), metric='euclidean'))),\n",
    "                       X_train_vectors.items()))\n",
    "\n",
    "# Same thing but between all elements of the training set and of the validation set\n",
    "X_val_dist = sum(map(lambda x: normalize(cdist(x[0][1].tolist(), x[1][1].tolist(), metric='euclidean')), \n",
    "                     zip(X_val_vectors.items(), X_train_vectors.items())))\n",
    "\n",
    "X_train_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric='precomputed')\n",
    "\n",
    "accuracy, auc = compute_scores(knn, X_train_dist, y_train, X_val_dist, y_val)\n",
    "print(\"k-nearest neighbors. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44012e7-95c4-44cc-98af-4a070164e638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "mlbd_lecture4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mdev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
