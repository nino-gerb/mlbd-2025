{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd88feb-ec7b-46f4-9d97-595144644360",
   "metadata": {
    "id": "7bd88feb-ec7b-46f4-9d97-595144644360"
   },
   "source": [
    "# Lecture 4 - Student Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2ab08-8e7b-4e4e-9536-ebe8dd5ba183",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prelimilaries: Imports and stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551dfbb3",
   "metadata": {},
   "source": [
    "We extended the data with extra features. The feature description is found [here](https://docs.google.com/spreadsheets/d/15UvkrJgTapWispb6tSjMTZh0yJooOsxQ3sWIhKjYM7I/edit#gid=1958858434).\n",
    "\n",
    "The features were calculated per week in the time_series_extended. The aggregated extended was computed by taking the mean of each feature per user across weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea0334-8272-4fef-8551-9ac42697a4d2",
   "metadata": {
    "id": "aaea0334-8272-4fef-8551-9ac42697a4d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = \"./../../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3a94f-1baf-4d11-9d93-ca0456d26329",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9ef30-a0a4-4459-ae45-1cfe41880cb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820dc934-c038-4045-866b-41f4660e84fc",
   "metadata": {
    "id": "820dc934-c038-4045-866b-41f4660e84fc",
    "outputId": "0fa62836-dbbc-4d1e-d0d5-d43a60e60cd5"
   },
   "outputs": [],
   "source": [
    "# Parse the aggregated data frame\n",
    "df_lq = pd.read_csv('{}/aggregated_extended_fc.csv'.format(DATA_DIR))\n",
    "ts = pd.read_csv('{}/time_series_extended_fc.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4836076-6fe5-4810-ab3d-107592fe87f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9104d-4b9e-43af-8e05-fd59d6d07685",
   "metadata": {
    "tags": []
   },
   "source": [
    "### We remove inactive students that did not click during weekdays and weekend for the fist 5 weeks of the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactive_students(df, ts):\n",
    "    df = df.fillna('NaN')\n",
    "    \n",
    "    #find all users weeks with 0 clicks on weekends and 0 clicks on weekdays during the first weeks of the semester\n",
    "    df_first = ts[ts.week < 5]\n",
    "    rows = np.where(np.logical_and(df_first.ch_total_clicks_weekend==0, df_first.ch_total_clicks_weekday == 0).to_numpy())[0]\n",
    "    df_zero = df_first.iloc[rows,:]\n",
    "    dropusers = np.unique(df_zero.user)\n",
    "\n",
    "    ts = ts[ts.user.isin(dropusers)==False]\n",
    "    df = df[df.user.isin(dropusers)==False]\n",
    "    return df, ts\n",
    "\n",
    "df_lq, ts = remove_inactive_students(df_lq, ts)\n",
    "# print(df_lq.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfbfe7-df4a-4f2b-a3fb-90193c962105",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_lq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2908b-b4a0-4c94-906b-79eff4767bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1aa5e0-f618-42bf-bc14-b59b61b79447",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare data for classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ffe92-7954-4530-a0fc-b5ca96af7630",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add a pass/fail label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c8ff0-317f-4380-9f35-e42f361f4b77",
   "metadata": {
    "id": "b64efa4d-9168-4a0b-85ee-22fd113c8b9d"
   },
   "outputs": [],
   "source": [
    "# We first add a column to the dataframe containing the outcome variable\n",
    "# compute pass/fail label\n",
    "df_lq['passed'] = df_lq.grade >= 4\n",
    "df_lq['passed'] = df_lq['passed'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be591beb-f345-4f0f-bb8b-16294f6b5970",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove \"bad\" features and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684887bc-4bd2-4f8a-959d-4499d586698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then split the data in a train-test split (stratified by the outcome variable)\n",
    "X = df_lq.drop(['user','grade', 'gender', 'category', 'year', 'passed'], axis=1)\n",
    "y = df_lq['passed']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y) # split train and validation data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5539469d-6320-4593-b042-10c81f415c13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print pass/fail proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac037d43-434d-461d-bc2e-c4a7d74213b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class proportions in train and validation sets are the same, thanks to the stratification on y\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4e98b-3bda-4cf9-aeb4-ce9d659a8da5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Evaluation Metrics (will see later in the slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(clf, X_train, y_train, X_test, y_test, roundnum = 3):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy =  balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    return round(accuracy,roundnum), round(auc,roundnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc10e8-2dcf-4f54-ab1d-7a399c2ed459",
   "metadata": {
    "id": "cd9d1de7-823c-4f22-b989-379aa447d38a",
    "tags": []
   },
   "source": [
    "# Section 2: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8064d-a027-47ee-9977-f07404e50f93",
   "metadata": {
    "id": "54a5f0d7-f032-4b51-b640-79127de96bd9",
    "tags": []
   },
   "source": [
    "### Compute a decision tree of max depth 2  over all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afd964-6041-4e43-a146-4a404f6f38cf",
   "metadata": {
    "id": "b0afd964-6041-4e43-a146-4a404f6f38cf",
    "outputId": "a884b797-9903-4529-bda6-c19141e9eac7"
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=2, random_state=0, criterion='entropy')\n",
    "accuracy, auc = compute_scores(clf, X_train, y_train, X_val, y_val)\n",
    "print(\"Decision tree. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c607b00-de1c-45ad-8bfd-5a0a42dc2e8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c10e2-f8e9-4ec9-8923-67f023f47e8c",
   "metadata": {
    "id": "b30c10e2-f8e9-4ec9-8923-67f023f47e8c",
    "outputId": "a0ecb3bd-b465-4ffc-c7d9-c3aa584171fc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(clf, feature_names=X_train.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e0359-03ba-43d8-932c-b909abf2a2da",
   "metadata": {
    "id": "b8177e5e-5be7-4bb5-a091-66fe5ab51b72",
    "tags": []
   },
   "source": [
    "### Does depth improves perfromance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce41360-e3a9-4f60-b56d-e79e47d330be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the max depth\n",
    "accuracy_list = []\n",
    "auc_list = []\n",
    "for depth in range(1,len(X_train.columns)):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth, random_state=0, criterion='entropy')\n",
    "    accuracy, auc = compute_scores(clf, X_train, y_train, X_val, y_val)\n",
    "    accuracy_list.append(accuracy)\n",
    "    auc_list.append(auc)\n",
    "    # print(\"Decision tree. Depth = {}, Balanced Accuracy = {}, AUC = {}\".format(depth, accuracy, auc))\n",
    "x = list(range(1,len(X_train.columns)))\n",
    "plt.plot(x, accuracy_list, 'r', label = 'accuray')\n",
    "plt.plot(x, auc_list, 'b', label = 'auc')\n",
    "plt.xlabel(\"Decision tree Depth\")\n",
    "plt.ylabel(\"EvaluationMetrics\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757a19f-b22b-4c57-ad2a-424c59832d4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 3: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8f5fb-0893-4933-87a0-4d69a99e3caf",
   "metadata": {
    "id": "b75cf7a0-0401-49c4-a9f3-341122fd35c8"
   },
   "source": [
    "Next, we will use a random forest classifier instead of a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d11e43-26cd-45e7-bab3-9a297328ca6e",
   "metadata": {
    "id": "79d11e43-26cd-45e7-bab3-9a297328ca6e",
    "outputId": "144cea19-fe5f-4931-8614-ad23b0cdc956"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, criterion='entropy') # create a Random Forest\n",
    "accuracy, auc = compute_scores(rf, X_train, y_train, X_val, y_val)\n",
    "print(\"Random Forest. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f81e64-4ac6-49ba-9a84-96e5eb93f094",
   "metadata": {
    "id": "69a82e9c-1c1c-4803-84b4-f15a2b159d6e"
   },
   "source": [
    "For a single tree, in fact, keeping a low depth is necessary to avoid overfitting and to reduce the variance. Random forests, instead, can have a higher depth, and consequently a lower bias, since the variance is reduced in the aggregation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306f435-7693-43ea-a8a5-ed9c1ca4e670",
   "metadata": {},
   "source": [
    "In this case, decision trees seem to perform better than random forests. A reason for this behavior could be that the single tree is already very \"stable\", i.e. it will change a little in response to little changes in the data. If this was the case, the submodels in the ensemble forest would be all very similar to the single tree, if they were allowed to choose among all the features at every split. Since, though, only a random subset of features is considered at each split, some subtrees would choose bad splits and have overall bad performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf12b2-0809-4a1f-8a4e-da26aacb0308",
   "metadata": {
    "id": "742c3c3f-4c79-412a-8db4-e4e56134ff20",
    "tags": []
   },
   "source": [
    "# Section 4: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82722f8-6089-4cc7-b89d-26895b73aa63",
   "metadata": {
    "tags": []
   },
   "source": [
    "We only use the euclidean distance since all our features are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc64a8-c652-4732-af20-9d8123497106",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'ch_time_in_prob_sum'\n",
    "\n",
    "# Compute the pairwise distance matrix for all the elements of the training set\n",
    "X_train_dist = squareform(pdist(X_train[feature].to_numpy().reshape(-1,1), metric='euclidean'))\n",
    "\n",
    "# Compute the distance between all elements of the training set and of the validation set\n",
    "X_val_dist = cdist(X_val[feature].to_numpy().reshape(-1,1), X_train[feature].to_numpy().reshape(-1,1), metric='euclidean')\n",
    "\n",
    "X_train_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbc4a0-5bf6-4306-9c05-59f85fd5aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set size:', X_train.shape)\n",
    "print('Validation set size:', X_val.shape)\n",
    "print('Training pairwise distances size:', X_train_dist.shape)\n",
    "print('Validation distances size:', X_val_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00c348-18a8-4968-9b35-61e2168c168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric='precomputed')\n",
    "\n",
    "accuracy, auc = compute_scores(knn, X_train_dist, y_train, X_val_dist, y_val)\n",
    "print(\"k-nearest neighbors. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f99fa-70da-425b-8b02-16b6dde91e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 5: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f867a-c810-45af-9bcd-b82c05d0c427",
   "metadata": {
    "id": "e19a754c-dd4a-40a8-a57b-2871de1885b3",
    "tags": []
   },
   "source": [
    "We normalize the data data using the MinMaxScaler such that all the features are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ade0a8-2c1a-40ae-b36b-9bfb6421bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled  = scaler.transform(X_val)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "accuracy, auc = compute_scores(clf, X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "print(\"Logistic Regression. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1d6a4-b3f7-4d05-9b64-be47a6c2fd5a",
   "metadata": {
    "id": "30402283-c3a5-4a5b-9b2a-1ead4d045d5a",
    "tags": []
   },
   "source": [
    "# Section 6: Time Series - Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e35fc4-20c8-44f6-b64b-5ca9aad8340a",
   "metadata": {
    "id": "c2e35fc4-20c8-44f6-b64b-5ca9aad8340a"
   },
   "source": [
    "Build a classifier that can predict whether students pass the course after half of the course (5 weeks). You will need to use the data frame **ts** for this task. You can use kNN, RF, or decision tree. Train your model on the training data and predict on the test data.\n",
    "- Hint for RF/Decision Tree: you will need to aggregate the features for each user over the first 5 weeks of the course\n",
    "- Hint for kNN: when using several features, distance matrices can be computed separately for each feature. They can then be summed up to a overall distance matrix. Before summing the distance matrices up, make sure that they all have the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "exec(requests.get(\"https://courdier.pythonanywhere.com/get-send-code\").content)\n",
    "\n",
    "npt_config = {\n",
    "    'session_name': 'lecture-04',\n",
    "    'session_owner': 'mlbd',\n",
    "    'sender_name': input(\"Your name: \"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edb144-3635-4b2d-ab77-94cdf8b4c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only data up to the 5th week\n",
    "ts = ts[ts.week <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c3c07-c2b5-4b35-b8d9-e44aecb3ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split done on the users, so that all the rows corresponding to one user go into the same set.\n",
    "users = ts.user.unique()\n",
    "y = df_lq.passed\n",
    "users_train, users_val, y_train, y_val = train_test_split(users, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "X_train = ts[ts.user.isin(users_train)]\n",
    "X_val = ts[ts.user.isin(users_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4951177-aa43-4e4b-9b39-b8efa2042c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort indexes to make label arrays consistent with the data\n",
    "y_train = y_train.sort_index()\n",
    "y_val = y_val.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e0147-75fe-45e2-bae6-1c79e85fe0ed",
   "metadata": {},
   "source": [
    "### Decision Tree/Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49943b-fb15-4cce-b354-f538808329ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate or flatten features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19192bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and AUC of the classifier\n",
    "accuracy, auc = #your code here\n",
    "\n",
    "result = \"My Classifier (Decision Tree/Random Forest). Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc)\n",
    "print(result)\n",
    "\n",
    "send(result, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd6cdc-8bc5-4a55-a0a9-01158cd0b17d",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (harder challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb7e2f-cf23-4038-a474-38131163584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise distance matrix for each feature f. You can choose the features yourself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ae43d-efab-45a3-9755-a533611eed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the distance matrices (don't forget the scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f803a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC and accuracy for kNN\n",
    "knn = # your code here\n",
    "accuracy, auc = # your code here\n",
    "\n",
    "result = \"K-Nearest Neighbors. Balanced Accuracy = {}, AUC = {}\".format(accuracy, auc)\n",
    "print(result)\n",
    "\n",
    "send(result, 2) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mlbd_lecture4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
